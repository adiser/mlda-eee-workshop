{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us do our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours_spent_studying</th>\n",
       "      <th>marks_attained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hours_spent_studying  marks_attained\n",
       "0                     1               2\n",
       "1                     2               4\n",
       "2                     3               6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('study_marks.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dummy dataset that we read, we can see that we have two columns. One is `hours_spent_studying` which dictates how many hours we spent studying. Another column is `marks_attained` which is the numeric grade we obtained after some hours of studying\n",
    "\n",
    "Easily as a human observer, we can immediately see the linear relationship that is exhibited in the dataset (1 extra hour of studying will warrant you 2 extra marks). But how do we teach a machine to learn this relationship?\n",
    "\n",
    "<img src='imgs/capture.png' width='600'>\n",
    "\n",
    "At this part of the workshop, I hope that you are quite familiar with the machine learning pipeline as explained in the previous workshop. In this part of the workshop, we are going to get a bit technical and discuss some (****cough***) mathematics and hopefully you will fully appreciate the concepts and process on how a training process is undertaken\n",
    "\n",
    "We frame the problem as follows, given a new input x: `hours_spent_studying`, can we generate a model that can predict y: `marks_attained` with acceptable accuracy? In short, we are trying to obtain the best `w` to fit our data.\n",
    "\n",
    "`w` can mean anything. In this case, it can mean the marks attained over a unitary amount of work (`marks_attained/work_done`). In machine learning jargons, we usually refer `w` as *weights* or *parameters* of our model\n",
    "\n",
    "<img src='imgs/capture1.png' width='600'>\n",
    "\n",
    "Well, lets start coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a random guess of `w`, assign variable `w` with an arbitrary floating point value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial guess might not be that good! \n",
    "\n",
    "<img src='imgs/capture2.png', width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know what value of `w` is the best one?\n",
    "\n",
    "<img src='imgs/capture4.png', width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure we can observe the following:\n",
    "- There are infinitely many values of `w` that we can try\n",
    "- The best value of `w` is the one that minimizes the error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['hours_spent_studying'].values\n",
    "y = df['marks_attained'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x * w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y):\n",
    "    y_pred = model(x)\n",
    "    return (y_pred - y) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out different values of `w`, compute its loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 0.0\n",
      "\t 1.00 2.00 0.00 4.00\n",
      "\t 2.00 4.00 0.00 16.00\n",
      "\t 3.00 6.00 0.00 36.00\n",
      "Mean Squared Error: 18.667\n",
      "w: 0.1\n",
      "\t 1.00 2.00 0.10 3.61\n",
      "\t 2.00 4.00 0.20 14.44\n",
      "\t 3.00 6.00 0.30 32.49\n",
      "Mean Squared Error: 16.847\n",
      "w: 0.2\n",
      "\t 1.00 2.00 0.20 3.24\n",
      "\t 2.00 4.00 0.40 12.96\n",
      "\t 3.00 6.00 0.60 29.16\n",
      "Mean Squared Error: 15.120\n",
      "w: 0.30000000000000004\n",
      "\t 1.00 2.00 0.30 2.89\n",
      "\t 2.00 4.00 0.60 11.56\n",
      "\t 3.00 6.00 0.90 26.01\n",
      "Mean Squared Error: 13.487\n",
      "w: 0.4\n",
      "\t 1.00 2.00 0.40 2.56\n",
      "\t 2.00 4.00 0.80 10.24\n",
      "\t 3.00 6.00 1.20 23.04\n",
      "Mean Squared Error: 11.947\n",
      "w: 0.5\n",
      "\t 1.00 2.00 0.50 2.25\n",
      "\t 2.00 4.00 1.00 9.00\n",
      "\t 3.00 6.00 1.50 20.25\n",
      "Mean Squared Error: 10.500\n",
      "w: 0.6000000000000001\n",
      "\t 1.00 2.00 0.60 1.96\n",
      "\t 2.00 4.00 1.20 7.84\n",
      "\t 3.00 6.00 1.80 17.64\n",
      "Mean Squared Error: 9.147\n",
      "w: 0.7000000000000001\n",
      "\t 1.00 2.00 0.70 1.69\n",
      "\t 2.00 4.00 1.40 6.76\n",
      "\t 3.00 6.00 2.10 15.21\n",
      "Mean Squared Error: 7.887\n",
      "w: 0.8\n",
      "\t 1.00 2.00 0.80 1.44\n",
      "\t 2.00 4.00 1.60 5.76\n",
      "\t 3.00 6.00 2.40 12.96\n",
      "Mean Squared Error: 6.720\n",
      "w: 0.9\n",
      "\t 1.00 2.00 0.90 1.21\n",
      "\t 2.00 4.00 1.80 4.84\n",
      "\t 3.00 6.00 2.70 10.89\n",
      "Mean Squared Error: 5.647\n",
      "w: 1.0\n",
      "\t 1.00 2.00 1.00 1.00\n",
      "\t 2.00 4.00 2.00 4.00\n",
      "\t 3.00 6.00 3.00 9.00\n",
      "Mean Squared Error: 4.667\n",
      "w: 1.1\n",
      "\t 1.00 2.00 1.10 0.81\n",
      "\t 2.00 4.00 2.20 3.24\n",
      "\t 3.00 6.00 3.30 7.29\n",
      "Mean Squared Error: 3.780\n",
      "w: 1.2000000000000002\n",
      "\t 1.00 2.00 1.20 0.64\n",
      "\t 2.00 4.00 2.40 2.56\n",
      "\t 3.00 6.00 3.60 5.76\n",
      "Mean Squared Error: 2.987\n",
      "w: 1.3\n",
      "\t 1.00 2.00 1.30 0.49\n",
      "\t 2.00 4.00 2.60 1.96\n",
      "\t 3.00 6.00 3.90 4.41\n",
      "Mean Squared Error: 2.287\n",
      "w: 1.4000000000000001\n",
      "\t 1.00 2.00 1.40 0.36\n",
      "\t 2.00 4.00 2.80 1.44\n",
      "\t 3.00 6.00 4.20 3.24\n",
      "Mean Squared Error: 1.680\n",
      "w: 1.5\n",
      "\t 1.00 2.00 1.50 0.25\n",
      "\t 2.00 4.00 3.00 1.00\n",
      "\t 3.00 6.00 4.50 2.25\n",
      "Mean Squared Error: 1.167\n",
      "w: 1.6\n",
      "\t 1.00 2.00 1.60 0.16\n",
      "\t 2.00 4.00 3.20 0.64\n",
      "\t 3.00 6.00 4.80 1.44\n",
      "Mean Squared Error: 0.747\n",
      "w: 1.7000000000000002\n",
      "\t 1.00 2.00 1.70 0.09\n",
      "\t 2.00 4.00 3.40 0.36\n",
      "\t 3.00 6.00 5.10 0.81\n",
      "Mean Squared Error: 0.420\n",
      "w: 1.8\n",
      "\t 1.00 2.00 1.80 0.04\n",
      "\t 2.00 4.00 3.60 0.16\n",
      "\t 3.00 6.00 5.40 0.36\n",
      "Mean Squared Error: 0.187\n",
      "w: 1.9000000000000001\n",
      "\t 1.00 2.00 1.90 0.01\n",
      "\t 2.00 4.00 3.80 0.04\n",
      "\t 3.00 6.00 5.70 0.09\n",
      "Mean Squared Error: 0.047\n",
      "w: 2.0\n",
      "\t 1.00 2.00 2.00 0.00\n",
      "\t 2.00 4.00 4.00 0.00\n",
      "\t 3.00 6.00 6.00 0.00\n",
      "Mean Squared Error: 0.000\n",
      "w: 2.1\n",
      "\t 1.00 2.00 2.10 0.01\n",
      "\t 2.00 4.00 4.20 0.04\n",
      "\t 3.00 6.00 6.30 0.09\n",
      "Mean Squared Error: 0.047\n",
      "w: 2.2\n",
      "\t 1.00 2.00 2.20 0.04\n",
      "\t 2.00 4.00 4.40 0.16\n",
      "\t 3.00 6.00 6.60 0.36\n",
      "Mean Squared Error: 0.187\n",
      "w: 2.3000000000000003\n",
      "\t 1.00 2.00 2.30 0.09\n",
      "\t 2.00 4.00 4.60 0.36\n",
      "\t 3.00 6.00 6.90 0.81\n",
      "Mean Squared Error: 0.420\n",
      "w: 2.4000000000000004\n",
      "\t 1.00 2.00 2.40 0.16\n",
      "\t 2.00 4.00 4.80 0.64\n",
      "\t 3.00 6.00 7.20 1.44\n",
      "Mean Squared Error: 0.747\n",
      "w: 2.5\n",
      "\t 1.00 2.00 2.50 0.25\n",
      "\t 2.00 4.00 5.00 1.00\n",
      "\t 3.00 6.00 7.50 2.25\n",
      "Mean Squared Error: 1.167\n",
      "w: 2.6\n",
      "\t 1.00 2.00 2.60 0.36\n",
      "\t 2.00 4.00 5.20 1.44\n",
      "\t 3.00 6.00 7.80 3.24\n",
      "Mean Squared Error: 1.680\n",
      "w: 2.7\n",
      "\t 1.00 2.00 2.70 0.49\n",
      "\t 2.00 4.00 5.40 1.96\n",
      "\t 3.00 6.00 8.10 4.41\n",
      "Mean Squared Error: 2.287\n",
      "w: 2.8000000000000003\n",
      "\t 1.00 2.00 2.80 0.64\n",
      "\t 2.00 4.00 5.60 2.56\n",
      "\t 3.00 6.00 8.40 5.76\n",
      "Mean Squared Error: 2.987\n",
      "w: 2.9000000000000004\n",
      "\t 1.00 2.00 2.90 0.81\n",
      "\t 2.00 4.00 5.80 3.24\n",
      "\t 3.00 6.00 8.70 7.29\n",
      "Mean Squared Error: 3.780\n",
      "w: 3.0\n",
      "\t 1.00 2.00 3.00 1.00\n",
      "\t 2.00 4.00 6.00 4.00\n",
      "\t 3.00 6.00 9.00 9.00\n",
      "Mean Squared Error: 4.667\n",
      "w: 3.1\n",
      "\t 1.00 2.00 3.10 1.21\n",
      "\t 2.00 4.00 6.20 4.84\n",
      "\t 3.00 6.00 9.30 10.89\n",
      "Mean Squared Error: 5.647\n",
      "w: 3.2\n",
      "\t 1.00 2.00 3.20 1.44\n",
      "\t 2.00 4.00 6.40 5.76\n",
      "\t 3.00 6.00 9.60 12.96\n",
      "Mean Squared Error: 6.720\n",
      "w: 3.3000000000000003\n",
      "\t 1.00 2.00 3.30 1.69\n",
      "\t 2.00 4.00 6.60 6.76\n",
      "\t 3.00 6.00 9.90 15.21\n",
      "Mean Squared Error: 7.887\n",
      "w: 3.4000000000000004\n",
      "\t 1.00 2.00 3.40 1.96\n",
      "\t 2.00 4.00 6.80 7.84\n",
      "\t 3.00 6.00 10.20 17.64\n",
      "Mean Squared Error: 9.147\n",
      "w: 3.5\n",
      "\t 1.00 2.00 3.50 2.25\n",
      "\t 2.00 4.00 7.00 9.00\n",
      "\t 3.00 6.00 10.50 20.25\n",
      "Mean Squared Error: 10.500\n",
      "w: 3.6\n",
      "\t 1.00 2.00 3.60 2.56\n",
      "\t 2.00 4.00 7.20 10.24\n",
      "\t 3.00 6.00 10.80 23.04\n",
      "Mean Squared Error: 11.947\n",
      "w: 3.7\n",
      "\t 1.00 2.00 3.70 2.89\n",
      "\t 2.00 4.00 7.40 11.56\n",
      "\t 3.00 6.00 11.10 26.01\n",
      "Mean Squared Error: 13.487\n",
      "w: 3.8000000000000003\n",
      "\t 1.00 2.00 3.80 3.24\n",
      "\t 2.00 4.00 7.60 12.96\n",
      "\t 3.00 6.00 11.40 29.16\n",
      "Mean Squared Error: 15.120\n",
      "w: 3.9000000000000004\n",
      "\t 1.00 2.00 3.90 3.61\n",
      "\t 2.00 4.00 7.80 14.44\n",
      "\t 3.00 6.00 11.70 32.49\n",
      "Mean Squared Error: 16.847\n",
      "w: 4.0\n",
      "\t 1.00 2.00 4.00 4.00\n",
      "\t 2.00 4.00 8.00 16.00\n",
      "\t 3.00 6.00 12.00 36.00\n",
      "Mean Squared Error: 18.667\n"
     ]
    }
   ],
   "source": [
    "w_list = []\n",
    "mse_list = []\n",
    "\n",
    "for w in np.arange(0.0, 4.1, 0.1):\n",
    "    print(\"w:\", w)\n",
    "    l_sum = 0\n",
    "    for x_val, y_val in zip(x, y):\n",
    "        y_pred_val = model(x_val)\n",
    "        l = loss(x_val, y_val)\n",
    "        l_sum += l\n",
    "        print('\\t {:.2f} {:.2f} {:.2f} {:.2f}'.format(x_val, y_val, y_pred_val, l))\n",
    "        \n",
    "    print(\"Mean Squared Error: {:.3f}\".format(l_sum/3))\n",
    "    w_list.append(w)\n",
    "    mse_list.append(l_sum/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'MSE')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x294517b2ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(w_list, mse_list)\n",
    "plt.xlabel('W')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we can observe that the best value of `w` is two as it yields the minimum error. Hence, how do we achieve the best value of `w` every time?:\n",
    "- Try out every possible values of `w`, \n",
    "- Choose the one that minimizes the error\n",
    "\n",
    "There are a few problems with this approach:\n",
    "- Too time consuming!\n",
    "- Hard to determine the range of values of `w` to try out\n",
    "- For problems involving multiple independent variables, have to increase the search space to determine multiple values of `w`\n",
    "\n",
    "There is a sure-get way to get to the optimum value of `w` i.e. the parameter of our model\n",
    "<img src='imgs/capture7.png', width=600>\n",
    "\n",
    "What we are essentially doing is making small steps to the direction that is dictated by the gradient (i.e. derivative) of loss function\n",
    "\n",
    "The following is the calculations that are involved to obtain the gradient of our loss function:\n",
    "\n",
    "<img src='imgs/capture10.png', width=400>\n",
    "\n",
    "# Implementation\n",
    "\n",
    "<img src='imgs/capture8.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our x and y values and our initial guess of w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['hours_spent_studying'].values\n",
    "y = df['marks_attained'].values\n",
    "\n",
    "w = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return w * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y):\n",
    "    y_pred = model(x)\n",
    "    return (y_pred - y) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y):\n",
    "    return 2 * x * (x * w - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of our model before training: 4.0\n",
      "\t x: 1 y: 2 grad: -2.0\n",
      "\t x: 2 y: 4 grad: -7.84\n",
      "\t x: 3 y: 6 grad: -16.2288\n",
      "epoch: 0 w: 1.260688, loss: 4.919240100095999\n",
      "\t x: 1 y: 2 grad: -1.478624\n",
      "\t x: 2 y: 4 grad: -5.796206079999999\n",
      "\t x: 3 y: 6 grad: -11.998146585599997\n",
      "epoch: 1 w: 1.453417766656, loss: 2.688769240265834\n",
      "\t x: 1 y: 2 grad: -1.093164466688\n",
      "\t x: 2 y: 4 grad: -4.285204709416961\n",
      "\t x: 3 y: 6 grad: -8.87037374849311\n",
      "epoch: 2 w: 1.5959051959019805, loss: 1.4696334962911515\n",
      "\t x: 1 y: 2 grad: -0.8081896081960389\n",
      "\t x: 2 y: 4 grad: -3.1681032641284723\n",
      "\t x: 3 y: 6 grad: -6.557973756745939\n",
      "epoch: 3 w: 1.701247862192685, loss: 0.8032755585999681\n",
      "\t x: 1 y: 2 grad: -0.59750427561463\n",
      "\t x: 2 y: 4 grad: -2.3422167604093502\n",
      "\t x: 3 y: 6 grad: -4.848388694047353\n",
      "epoch: 4 w: 1.7791289594933983, loss: 0.43905614881022015\n",
      "\t x: 1 y: 2 grad: -0.44174208101320334\n",
      "\t x: 2 y: 4 grad: -1.7316289575717576\n",
      "\t x: 3 y: 6 grad: -3.584471942173538\n",
      "epoch: 5 w: 1.836707389300983, loss: 0.2399802903801062\n",
      "\t x: 1 y: 2 grad: -0.3265852213980338\n",
      "\t x: 2 y: 4 grad: -1.2802140678802925\n",
      "\t x: 3 y: 6 grad: -2.650043120512205\n",
      "epoch: 6 w: 1.8792758133988885, loss: 0.1311689630744999\n",
      "\t x: 1 y: 2 grad: -0.241448373202223\n",
      "\t x: 2 y: 4 grad: -0.946477622952715\n",
      "\t x: 3 y: 6 grad: -1.9592086795121197\n",
      "epoch: 7 w: 1.910747160155559, loss: 0.07169462478267678\n",
      "\t x: 1 y: 2 grad: -0.17850567968888198\n",
      "\t x: 2 y: 4 grad: -0.6997422643804168\n",
      "\t x: 3 y: 6 grad: -1.4484664872674653\n",
      "epoch: 8 w: 1.9340143044689266, loss: 0.03918700813247573\n",
      "\t x: 1 y: 2 grad: -0.13197139106214673\n",
      "\t x: 2 y: 4 grad: -0.5173278529636143\n",
      "\t x: 3 y: 6 grad: -1.0708686556346834\n",
      "epoch: 9 w: 1.9512159834655312, loss: 0.021418922423117836\n",
      "\t x: 1 y: 2 grad: -0.09756803306893769\n",
      "\t x: 2 y: 4 grad: -0.38246668963023644\n",
      "\t x: 3 y: 6 grad: -0.7917060475345892\n",
      "epoch: 10 w: 1.9639333911678687, loss: 0.01170720245384975\n",
      "\t x: 1 y: 2 grad: -0.07213321766426262\n",
      "\t x: 2 y: 4 grad: -0.2827622132439096\n",
      "\t x: 3 y: 6 grad: -0.5853177814148953\n",
      "epoch: 11 w: 1.9733355232910992, loss: 0.006398948863435593\n",
      "\t x: 1 y: 2 grad: -0.05332895341780164\n",
      "\t x: 2 y: 4 grad: -0.2090494973977819\n",
      "\t x: 3 y: 6 grad: -0.4327324596134101\n",
      "epoch: 12 w: 1.9802866323953892, loss: 0.003497551760830656\n",
      "\t x: 1 y: 2 grad: -0.039426735209221686\n",
      "\t x: 2 y: 4 grad: -0.15455280202014876\n",
      "\t x: 3 y: 6 grad: -0.3199243001817109\n",
      "epoch: 13 w: 1.9854256707695, loss: 0.001911699652671057\n",
      "\t x: 1 y: 2 grad: -0.02914865846100012\n",
      "\t x: 2 y: 4 grad: -0.11426274116712065\n",
      "\t x: 3 y: 6 grad: -0.2365238742159388\n",
      "epoch: 14 w: 1.9892250235079405, loss: 0.0010449010656399273\n",
      "\t x: 1 y: 2 grad: -0.021549952984118992\n",
      "\t x: 2 y: 4 grad: -0.08447581569774698\n",
      "\t x: 3 y: 6 grad: -0.17486493849433593\n",
      "epoch: 15 w: 1.9920339305797026, loss: 0.0005711243580809696\n",
      "\t x: 1 y: 2 grad: -0.015932138840594856\n",
      "\t x: 2 y: 4 grad: -0.062453984255132156\n",
      "\t x: 3 y: 6 grad: -0.12927974740812687\n",
      "epoch: 16 w: 1.994110589284741, loss: 0.0003121664271570621\n",
      "\t x: 1 y: 2 grad: -0.011778821430517894\n",
      "\t x: 2 y: 4 grad: -0.046172980007630926\n",
      "\t x: 3 y: 6 grad: -0.09557806861579543\n",
      "epoch: 17 w: 1.9956458879852805, loss: 0.0001706246229305199\n",
      "\t x: 1 y: 2 grad: -0.008708224029438938\n",
      "\t x: 2 y: 4 grad: -0.03413623819540135\n",
      "\t x: 3 y: 6 grad: -0.07066201306448505\n",
      "epoch: 18 w: 1.9967809527381737, loss: 9.326038746484765e-05\n",
      "\t x: 1 y: 2 grad: -0.006438094523652627\n",
      "\t x: 2 y: 4 grad: -0.02523733053271826\n",
      "\t x: 3 y: 6 grad: -0.052241274202728505\n",
      "epoch: 19 w: 1.9976201197307648, loss: 5.097447086306101e-05\n",
      "\t x: 1 y: 2 grad: -0.004759760538470381\n",
      "\t x: 2 y: 4 grad: -0.01865826131080439\n",
      "\t x: 3 y: 6 grad: -0.03862260091336722\n",
      "epoch: 20 w: 1.998240525958391, loss: 2.7861740127856012e-05\n",
      "\t x: 1 y: 2 grad: -0.0035189480832178432\n",
      "\t x: 2 y: 4 grad: -0.01379427648621423\n",
      "\t x: 3 y: 6 grad: -0.028554152326460525\n",
      "epoch: 21 w: 1.99869919972735, loss: 1.5228732143933469e-05\n",
      "\t x: 1 y: 2 grad: -0.002601600545300009\n",
      "\t x: 2 y: 4 grad: -0.01019827413757568\n",
      "\t x: 3 y: 6 grad: -0.021110427464781978\n",
      "epoch: 22 w: 1.9990383027488265, loss: 8.323754426231206e-06\n",
      "\t x: 1 y: 2 grad: -0.001923394502346909\n",
      "\t x: 2 y: 4 grad: -0.007539706449199102\n",
      "\t x: 3 y: 6 grad: -0.01560719234984198\n",
      "epoch: 23 w: 1.9992890056818404, loss: 4.549616284094891e-06\n",
      "\t x: 1 y: 2 grad: -0.0014219886363191492\n",
      "\t x: 2 y: 4 grad: -0.005574195454370212\n",
      "\t x: 3 y: 6 grad: -0.011538584590544687\n",
      "epoch: 24 w: 1.999474353368653, loss: 2.486739429417538e-06\n",
      "\t x: 1 y: 2 grad: -0.0010512932626940419\n",
      "\t x: 2 y: 4 grad: -0.004121069589761106\n",
      "\t x: 3 y: 6 grad: -0.008530614050808794\n",
      "epoch: 25 w: 1.9996113831376856, loss: 1.3592075910762856e-06\n",
      "\t x: 1 y: 2 grad: -0.0007772337246287897\n",
      "\t x: 2 y: 4 grad: -0.0030467562005451754\n",
      "\t x: 3 y: 6 grad: -0.006306785335127074\n",
      "epoch: 26 w: 1.9997126908902887, loss: 7.429187207079447e-07\n",
      "\t x: 1 y: 2 grad: -0.0005746182194226179\n",
      "\t x: 2 y: 4 grad: -0.002252503420136165\n",
      "\t x: 3 y: 6 grad: -0.00466268207967957\n",
      "epoch: 27 w: 1.9997875889274812, loss: 4.060661735575354e-07\n",
      "\t x: 1 y: 2 grad: -0.0004248221450375844\n",
      "\t x: 2 y: 4 grad: -0.0016653028085471533\n",
      "\t x: 3 y: 6 grad: -0.0034471768136938863\n",
      "epoch: 28 w: 1.9998429619451539, loss: 2.2194855602869353e-07\n",
      "\t x: 1 y: 2 grad: -0.00031407610969225175\n",
      "\t x: 2 y: 4 grad: -0.0012311783499932005\n",
      "\t x: 3 y: 6 grad: -0.0025485391844828342\n",
      "epoch: 29 w: 1.9998838998815958, loss: 1.213131374411496e-07\n",
      "\t x: 1 y: 2 grad: -0.00023220023680847746\n",
      "\t x: 2 y: 4 grad: -0.0009102249282886277\n",
      "\t x: 3 y: 6 grad: -0.0018841656015560204\n",
      "epoch: 30 w: 1.9999141657892625, loss: 6.630760559646474e-08\n",
      "\t x: 1 y: 2 grad: -0.00017166842147497974\n",
      "\t x: 2 y: 4 grad: -0.0006729402121816719\n",
      "\t x: 3 y: 6 grad: -0.0013929862392156878\n",
      "epoch: 31 w: 1.9999365417379913, loss: 3.624255915449335e-08\n",
      "\t x: 1 y: 2 grad: -0.0001269165240174175\n",
      "\t x: 2 y: 4 grad: -0.0004975127741477792\n",
      "\t x: 3 y: 6 grad: -0.0010298514424817995\n",
      "epoch: 32 w: 1.9999530845453979, loss: 1.9809538924707548e-08\n",
      "\t x: 1 y: 2 grad: -9.383090920422887e-05\n",
      "\t x: 2 y: 4 grad: -0.00036781716408107457\n",
      "\t x: 3 y: 6 grad: -0.0007613815296476645\n",
      "epoch: 33 w: 1.9999653148414271, loss: 1.0827542027017377e-08\n",
      "\t x: 1 y: 2 grad: -6.937031714571162e-05\n",
      "\t x: 2 y: 4 grad: -0.0002719316432120422\n",
      "\t x: 3 y: 6 grad: -0.0005628985014531906\n",
      "epoch: 34 w: 1.999974356846045, loss: 5.9181421028034105e-09\n",
      "\t x: 1 y: 2 grad: -5.1286307909848006e-05\n",
      "\t x: 2 y: 4 grad: -0.00020104232700646207\n",
      "\t x: 3 y: 6 grad: -0.0004161576169003922\n",
      "epoch: 35 w: 1.9999810417085633, loss: 3.2347513278475087e-09\n",
      "\t x: 1 y: 2 grad: -3.7916582873442906e-05\n",
      "\t x: 2 y: 4 grad: -0.0001486330048638962\n",
      "\t x: 3 y: 6 grad: -0.0003076703200690645\n",
      "epoch: 36 w: 1.9999859839076413, loss: 1.7680576050779005e-09\n",
      "\t x: 1 y: 2 grad: -2.8032184717474706e-05\n",
      "\t x: 2 y: 4 grad: -0.0001098861640933535\n",
      "\t x: 3 y: 6 grad: -0.00022746435967313516\n",
      "epoch: 37 w: 1.9999896377347262, loss: 9.6638887447731e-10\n",
      "\t x: 1 y: 2 grad: -2.0724530547688857e-05\n",
      "\t x: 2 y: 4 grad: -8.124015974608767e-05\n",
      "\t x: 3 y: 6 grad: -0.00016816713067413502\n",
      "epoch: 38 w: 1.999992339052936, loss: 5.282109892545845e-10\n",
      "\t x: 1 y: 2 grad: -1.5321894128117464e-05\n",
      "\t x: 2 y: 4 grad: -6.006182498197177e-05\n",
      "\t x: 3 y: 6 grad: -0.00012432797771566584\n",
      "epoch: 39 w: 1.9999943361699042, loss: 2.887107421958329e-10\n",
      "\t x: 1 y: 2 grad: -1.1327660191629008e-05\n",
      "\t x: 2 y: 4 grad: -4.4404427951505454e-05\n",
      "\t x: 3 y: 6 grad: -9.191716585732479e-05\n",
      "epoch: 40 w: 1.9999958126624442, loss: 1.5780416225633037e-10\n",
      "\t x: 1 y: 2 grad: -8.37467511161094e-06\n",
      "\t x: 2 y: 4 grad: -3.282872643772805e-05\n",
      "\t x: 3 y: 6 grad: -6.795546372551087e-05\n",
      "epoch: 41 w: 1.999996904251097, loss: 8.625295142578772e-11\n",
      "\t x: 1 y: 2 grad: -6.191497806007362e-06\n",
      "\t x: 2 y: 4 grad: -2.4270671399762023e-05\n",
      "\t x: 3 y: 6 grad: -5.0240289795056015e-05\n",
      "epoch: 42 w: 1.999997711275687, loss: 4.71443308235547e-11\n",
      "\t x: 1 y: 2 grad: -4.5774486259198e-06\n",
      "\t x: 2 y: 4 grad: -1.794359861406747e-05\n",
      "\t x: 3 y: 6 grad: -3.714324913239864e-05\n",
      "epoch: 43 w: 1.9999983079186507, loss: 2.5768253628059826e-11\n",
      "\t x: 1 y: 2 grad: -3.3841626985164908e-06\n",
      "\t x: 2 y: 4 grad: -1.326591777761621e-05\n",
      "\t x: 3 y: 6 grad: -2.7460449796734565e-05\n",
      "epoch: 44 w: 1.9999987490239537, loss: 1.4084469615916932e-11\n",
      "\t x: 1 y: 2 grad: -2.5019520926150562e-06\n",
      "\t x: 2 y: 4 grad: -9.807652203264183e-06\n",
      "\t x: 3 y: 6 grad: -2.0301840059744336e-05\n",
      "epoch: 45 w: 1.9999990751383971, loss: 7.698320862431846e-12\n",
      "\t x: 1 y: 2 grad: -1.8497232057157476e-06\n",
      "\t x: 2 y: 4 grad: -7.250914967116273e-06\n",
      "\t x: 3 y: 6 grad: -1.5009393983689279e-05\n",
      "epoch: 46 w: 1.9999993162387186, loss: 4.20776540913866e-12\n",
      "\t x: 1 y: 2 grad: -1.3675225627451937e-06\n",
      "\t x: 2 y: 4 grad: -5.3606884460322135e-06\n",
      "\t x: 3 y: 6 grad: -1.109662508014253e-05\n",
      "epoch: 47 w: 1.9999994944870796, loss: 2.299889814334344e-12\n",
      "\t x: 1 y: 2 grad: -1.0110258408246864e-06\n",
      "\t x: 2 y: 4 grad: -3.963221296032771e-06\n",
      "\t x: 3 y: 6 grad: -8.20386808086937e-06\n",
      "epoch: 48 w: 1.9999996262682318, loss: 1.2570789110540446e-12\n",
      "\t x: 1 y: 2 grad: -7.474635363990956e-07\n",
      "\t x: 2 y: 4 grad: -2.930057062755509e-06\n",
      "\t x: 3 y: 6 grad: -6.065218119744031e-06\n",
      "epoch: 49 w: 1.999999723695619, loss: 6.870969979249939e-13\n",
      "\t x: 1 y: 2 grad: -5.526087618612507e-07\n",
      "\t x: 2 y: 4 grad: -2.166226346744793e-06\n",
      "\t x: 3 y: 6 grad: -4.484088535150477e-06\n",
      "epoch: 50 w: 1.9999997957248556, loss: 3.7555501141274804e-13\n",
      "\t x: 1 y: 2 grad: -4.08550288710785e-07\n",
      "\t x: 2 y: 4 grad: -1.6015171322436572e-06\n",
      "\t x: 3 y: 6 grad: -3.3151404608133817e-06\n",
      "epoch: 51 w: 1.9999998489769344, loss: 2.052716967104274e-13\n",
      "\t x: 1 y: 2 grad: -3.020461312175371e-07\n",
      "\t x: 2 y: 4 grad: -1.1840208351543424e-06\n",
      "\t x: 3 y: 6 grad: -2.4509231284497446e-06\n",
      "epoch: 52 w: 1.9999998883468353, loss: 1.1219786256679713e-13\n",
      "\t x: 1 y: 2 grad: -2.2330632942768602e-07\n",
      "\t x: 2 y: 4 grad: -8.753608113920563e-07\n",
      "\t x: 3 y: 6 grad: -1.811996877876254e-06\n",
      "epoch: 53 w: 1.9999999174534755, loss: 6.132535848018759e-14\n",
      "\t x: 1 y: 2 grad: -1.6509304900935717e-07\n",
      "\t x: 2 y: 4 grad: -6.471647520100987e-07\n",
      "\t x: 3 y: 6 grad: -1.3396310407642886e-06\n",
      "epoch: 54 w: 1.999999938972364, loss: 3.351935118167793e-14\n",
      "\t x: 1 y: 2 grad: -1.220552721115098e-07\n",
      "\t x: 2 y: 4 grad: -4.784566662863199e-07\n",
      "\t x: 3 y: 6 grad: -9.904052991061008e-07\n",
      "epoch: 55 w: 1.9999999548815364, loss: 1.8321081844499955e-14\n",
      "\t x: 1 y: 2 grad: -9.023692726373156e-08\n",
      "\t x: 2 y: 4 grad: -3.5372875473171916e-07\n",
      "\t x: 3 y: 6 grad: -7.322185204827747e-07\n",
      "epoch: 56 w: 1.9999999666433785, loss: 1.0013977760018664e-14\n",
      "\t x: 1 y: 2 grad: -6.671324292994996e-08\n",
      "\t x: 2 y: 4 grad: -2.615159129248923e-07\n",
      "\t x: 3 y: 6 grad: -5.413379398078177e-07\n",
      "epoch: 57 w: 1.9999999753390494, loss: 5.473462367088053e-15\n",
      "\t x: 1 y: 2 grad: -4.932190122985958e-08\n",
      "\t x: 2 y: 4 grad: -1.9334185274999527e-07\n",
      "\t x: 3 y: 6 grad: -4.002176350326181e-07\n",
      "epoch: 58 w: 1.9999999817678633, loss: 2.991697274308627e-15\n",
      "\t x: 1 y: 2 grad: -3.6464273378555845e-08\n",
      "\t x: 2 y: 4 grad: -1.429399514307761e-07\n",
      "\t x: 3 y: 6 grad: -2.9588569994132286e-07\n",
      "epoch: 59 w: 1.9999999865207625, loss: 1.6352086111474931e-15\n",
      "\t x: 1 y: 2 grad: -2.6958475007887728e-08\n",
      "\t x: 2 y: 4 grad: -1.0567722164012139e-07\n",
      "\t x: 3 y: 6 grad: -2.1875184863517916e-07\n",
      "epoch: 60 w: 1.999999990034638, loss: 8.937759877335403e-16\n",
      "\t x: 1 y: 2 grad: -1.993072418216002e-08\n",
      "\t x: 2 y: 4 grad: -7.812843882959442e-08\n",
      "\t x: 3 y: 6 grad: -1.617258700292723e-07\n",
      "epoch: 61 w: 1.9999999926324883, loss: 4.885220495987371e-16\n",
      "\t x: 1 y: 2 grad: -1.473502342363986e-08\n",
      "\t x: 2 y: 4 grad: -5.7761292637792394e-08\n",
      "\t x: 3 y: 6 grad: -1.195658771990793e-07\n",
      "epoch: 62 w: 1.99999999455311, loss: 2.670175009618106e-16\n",
      "\t x: 1 y: 2 grad: -1.0893780100218464e-08\n",
      "\t x: 2 y: 4 grad: -4.270361841918202e-08\n",
      "\t x: 3 y: 6 grad: -8.839649012770678e-08\n",
      "epoch: 63 w: 1.9999999959730488, loss: 1.4594702493172377e-16\n",
      "\t x: 1 y: 2 grad: -8.05390243385773e-09\n",
      "\t x: 2 y: 4 grad: -3.1571296688071016e-08\n",
      "\t x: 3 y: 6 grad: -6.53525820126788e-08\n",
      "epoch: 64 w: 1.9999999970228268, loss: 7.977204100704301e-17\n",
      "\t x: 1 y: 2 grad: -5.9543463493128e-09\n",
      "\t x: 2 y: 4 grad: -2.334103754719763e-08\n",
      "\t x: 3 y: 6 grad: -4.8315948575350376e-08\n",
      "epoch: 65 w: 1.9999999977989402, loss: 4.360197735196887e-17\n",
      "\t x: 1 y: 2 grad: -4.402119557767037e-09\n",
      "\t x: 2 y: 4 grad: -1.725630838222969e-08\n",
      "\t x: 3 y: 6 grad: -3.5720557178819945e-08\n",
      "epoch: 66 w: 1.9999999983727301, loss: 2.3832065197304227e-17\n",
      "\t x: 1 y: 2 grad: -3.254539748809293e-09\n",
      "\t x: 2 y: 4 grad: -1.2757796596929438e-08\n",
      "\t x: 3 y: 6 grad: -2.6408640607655798e-08\n",
      "epoch: 67 w: 1.9999999987969397, loss: 1.3026183953845832e-17\n",
      "\t x: 1 y: 2 grad: -2.406120636067044e-09\n",
      "\t x: 2 y: 4 grad: -9.431992964437086e-09\n",
      "\t x: 3 y: 6 grad: -1.9524227568012975e-08\n",
      "epoch: 68 w: 1.999999999110563, loss: 7.11988308874388e-18\n",
      "\t x: 1 y: 2 grad: -1.7788739370416806e-09\n",
      "\t x: 2 y: 4 grad: -6.97318647269185e-09\n",
      "\t x: 3 y: 6 grad: -1.4434496264925656e-08\n",
      "epoch: 69 w: 1.9999999993424284, loss: 3.89160224698574e-18\n",
      "\t x: 1 y: 2 grad: -1.3151431055291596e-09\n",
      "\t x: 2 y: 4 grad: -5.155360582875801e-09\n",
      "\t x: 3 y: 6 grad: -1.067159693945996e-08\n",
      "epoch: 70 w: 1.9999999995138495, loss: 2.1270797208746147e-18\n",
      "\t x: 1 y: 2 grad: -9.72300906454393e-10\n",
      "\t x: 2 y: 4 grad: -3.811418736177075e-09\n",
      "\t x: 3 y: 6 grad: -7.88963561149103e-09\n",
      "epoch: 71 w: 1.9999999996405833, loss: 1.1626238773828175e-18\n",
      "\t x: 1 y: 2 grad: -7.18833437218791e-10\n",
      "\t x: 2 y: 4 grad: -2.8178277489132597e-09\n",
      "\t x: 3 y: 6 grad: -5.832902161273523e-09\n",
      "epoch: 72 w: 1.999999999734279, loss: 6.354692062078993e-19\n",
      "\t x: 1 y: 2 grad: -5.314420015167798e-10\n",
      "\t x: 2 y: 4 grad: -2.0832526814729135e-09\n",
      "\t x: 3 y: 6 grad: -4.31233715403323e-09\n",
      "epoch: 73 w: 1.9999999998035491, loss: 3.4733644793346653e-19\n",
      "\t x: 1 y: 2 grad: -3.92901711165905e-10\n",
      "\t x: 2 y: 4 grad: -1.5401742103904326e-09\n",
      "\t x: 3 y: 6 grad: -3.188159070077745e-09\n",
      "epoch: 74 w: 1.9999999998547615, loss: 1.8984796531526204e-19\n",
      "\t x: 1 y: 2 grad: -2.9047697580608656e-10\n",
      "\t x: 2 y: 4 grad: -1.1386696030513122e-09\n",
      "\t x: 3 y: 6 grad: -2.3570478902001923e-09\n",
      "epoch: 75 w: 1.9999999998926234, loss: 1.0376765851119951e-19\n",
      "\t x: 1 y: 2 grad: -2.1475310418850313e-10\n",
      "\t x: 2 y: 4 grad: -8.418314934033333e-10\n",
      "\t x: 3 y: 6 grad: -1.7425900722400911e-09\n",
      "epoch: 76 w: 1.9999999999206153, loss: 5.671751114309842e-20\n",
      "\t x: 1 y: 2 grad: -1.5876944203796484e-10\n",
      "\t x: 2 y: 4 grad: -6.223768167501476e-10\n",
      "\t x: 3 y: 6 grad: -1.2883241140571045e-09\n",
      "epoch: 77 w: 1.9999999999413098, loss: 3.100089617511693e-20\n",
      "\t x: 1 y: 2 grad: -1.17380327679939e-10\n",
      "\t x: 2 y: 4 grad: -4.601314884666863e-10\n",
      "\t x: 3 y: 6 grad: -9.524754318590567e-10\n",
      "epoch: 78 w: 1.9999999999566096, loss: 1.6944600977692705e-20\n",
      "\t x: 1 y: 2 grad: -8.678080476443029e-11\n",
      "\t x: 2 y: 4 grad: -3.4018121652934497e-10\n",
      "\t x: 3 y: 6 grad: -7.041780492045291e-10\n",
      "epoch: 79 w: 1.9999999999679208, loss: 9.2616919156479e-21\n",
      "\t x: 1 y: 2 grad: -6.415845632545825e-11\n",
      "\t x: 2 y: 4 grad: -2.5150193039280566e-10\n",
      "\t x: 3 y: 6 grad: -5.206075570640678e-10\n",
      "epoch: 80 w: 1.9999999999762834, loss: 5.062350511130293e-21\n",
      "\t x: 1 y: 2 grad: -4.743316850408519e-11\n",
      "\t x: 2 y: 4 grad: -1.8593837580738182e-10\n",
      "\t x: 3 y: 6 grad: -3.8489211817704927e-10\n",
      "epoch: 81 w: 1.999999999982466, loss: 2.7669155644059242e-21\n",
      "\t x: 1 y: 2 grad: -3.5067948545020045e-11\n",
      "\t x: 2 y: 4 grad: -1.3746692673066718e-10\n",
      "\t x: 3 y: 6 grad: -2.845563784603655e-10\n",
      "epoch: 82 w: 1.9999999999870368, loss: 1.5124150106147723e-21\n",
      "\t x: 1 y: 2 grad: -2.5926372160256506e-11\n",
      "\t x: 2 y: 4 grad: -1.0163070385260653e-10\n",
      "\t x: 3 y: 6 grad: -2.1037571684701106e-10\n",
      "epoch: 83 w: 1.999999999990416, loss: 8.26683933105326e-22\n",
      "\t x: 1 y: 2 grad: -1.9167778475548403e-11\n",
      "\t x: 2 y: 4 grad: -7.51381179497912e-11\n",
      "\t x: 3 y: 6 grad: -1.5553425214420713e-10\n",
      "epoch: 84 w: 1.9999999999929146, loss: 4.518126871054872e-22\n",
      "\t x: 1 y: 2 grad: -1.4170886686315498e-11\n",
      "\t x: 2 y: 4 grad: -5.555023108172463e-11\n",
      "\t x: 3 y: 6 grad: -1.1499068364173581e-10\n",
      "epoch: 85 w: 1.9999999999947617, loss: 2.469467919185614e-22\n",
      "\t x: 1 y: 2 grad: -1.0476508549572827e-11\n",
      "\t x: 2 y: 4 grad: -4.106759377009439e-11\n",
      "\t x: 3 y: 6 grad: -8.500933290633839e-11\n",
      "epoch: 86 w: 1.9999999999961273, loss: 1.349840097651456e-22\n",
      "\t x: 1 y: 2 grad: -7.745359908994942e-12\n",
      "\t x: 2 y: 4 grad: -3.036149109902908e-11\n",
      "\t x: 3 y: 6 grad: -6.285105769165966e-11\n",
      "epoch: 87 w: 1.999999999997137, loss: 7.376551550022107e-23\n",
      "\t x: 1 y: 2 grad: -5.726086271806707e-12\n",
      "\t x: 2 y: 4 grad: -2.2446045022661565e-11\n",
      "\t x: 3 y: 6 grad: -4.646416584819235e-11\n",
      "epoch: 88 w: 1.9999999999978835, loss: 4.031726170507742e-23\n",
      "\t x: 1 y: 2 grad: -4.233058348290797e-12\n",
      "\t x: 2 y: 4 grad: -1.659294923683774e-11\n",
      "\t x: 3 y: 6 grad: -3.4351188560322043e-11\n",
      "epoch: 89 w: 1.9999999999984353, loss: 2.2033851437431755e-23\n",
      "\t x: 1 y: 2 grad: -3.1294966618133913e-12\n",
      "\t x: 2 y: 4 grad: -1.226752033289813e-11\n",
      "\t x: 3 y: 6 grad: -2.539835008974478e-11\n",
      "epoch: 90 w: 1.9999999999988431, loss: 1.2047849775995315e-23\n",
      "\t x: 1 y: 2 grad: -2.3137047833188262e-12\n",
      "\t x: 2 y: 4 grad: -9.070078021977679e-12\n",
      "\t x: 3 y: 6 grad: -1.8779644506139448e-11\n",
      "epoch: 91 w: 1.9999999999991447, loss: 6.5840863393251405e-24\n",
      "\t x: 1 y: 2 grad: -1.7106316363424412e-12\n",
      "\t x: 2 y: 4 grad: -6.7057470687359455e-12\n",
      "\t x: 3 y: 6 grad: -1.3882228699912957e-11\n",
      "epoch: 92 w: 1.9999999999993676, loss: 3.5991747246272455e-24\n",
      "\t x: 1 y: 2 grad: -1.2647660696529783e-12\n",
      "\t x: 2 y: 4 grad: -4.957811938766099e-12\n",
      "\t x: 3 y: 6 grad: -1.0263789818054647e-11\n",
      "epoch: 93 w: 1.9999999999995324, loss: 1.969312363793734e-24\n",
      "\t x: 1 y: 2 grad: -9.352518759442319e-13\n",
      "\t x: 2 y: 4 grad: -3.666400516522117e-12\n",
      "\t x: 3 y: 6 grad: -7.58859641791787e-12\n",
      "epoch: 94 w: 1.9999999999996543, loss: 1.0761829795642296e-24\n",
      "\t x: 1 y: 2 grad: -6.914468997365475e-13\n",
      "\t x: 2 y: 4 grad: -2.7107205369247822e-12\n",
      "\t x: 3 y: 6 grad: -5.611511255665391e-12\n",
      "epoch: 95 w: 1.9999999999997444, loss: 5.875191475205477e-25\n",
      "\t x: 1 y: 2 grad: -5.111466805374221e-13\n",
      "\t x: 2 y: 4 grad: -2.0037305148434825e-12\n",
      "\t x: 3 y: 6 grad: -4.1460168631601846e-12\n",
      "epoch: 96 w: 1.999999999999811, loss: 3.2110109830478153e-25\n",
      "\t x: 1 y: 2 grad: -3.779199175824033e-13\n",
      "\t x: 2 y: 4 grad: -1.4814816040598089e-12\n",
      "\t x: 3 y: 6 grad: -3.064215547965432e-12\n",
      "epoch: 97 w: 1.9999999999998603, loss: 1.757455879087579e-25\n",
      "\t x: 1 y: 2 grad: -2.793321129956894e-13\n",
      "\t x: 2 y: 4 grad: -1.0942358130705543e-12\n",
      "\t x: 3 y: 6 grad: -2.2648549702353193e-12\n",
      "epoch: 98 w: 1.9999999999998967, loss: 9.608404711682446e-26\n",
      "\t x: 1 y: 2 grad: -2.0650148258027912e-13\n",
      "\t x: 2 y: 4 grad: -8.100187187665142e-13\n",
      "\t x: 3 y: 6 grad: -1.6786572132332367e-12\n",
      "epoch: 99 w: 1.9999999999999236, loss: 5.250973729513143e-26\n",
      "Prediction of our model after training: 7.9999999999996945\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction of our model before training: {}\".format(model(4)))\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x_val, y_val in zip(x, y):\n",
    "        grad = gradient(x_val, y_val)\n",
    "        w = w - 0.01 * grad\n",
    "        print(\"\\t x: {} y: {} grad: {}\".format(x_val, y_val, grad))\n",
    "        l = loss(x_val, y_val)\n",
    "        \n",
    "    print(\"epoch: {} w: {}, loss: {}\".format(epoch, w, l))\n",
    "    \n",
    "print(\"Prediction of our model after training: {}\".format(model(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to sklearn API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you're probably thinking:\n",
    "*\"Bro, ML is so hard! So many math! Do I need to do all this when I build a model\"*\n",
    "\n",
    "Well the answer to that is: Of course not!\n",
    "\n",
    "A lot of people have dedicated their development careers to develop *frameworks* which are essentially tools for you to do amazing stuff. One of which is scikit-learn, which is a framework/API for us to develop machine learning models at ease with some degree of modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import scikit-learn's built in linear regression model to our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As easy as 1...2...3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted value is [8.]\n"
     ]
    }
   ],
   "source": [
    "x = x.reshape(-1,1)\n",
    "\n",
    "linreg.fit(x,y)\n",
    "prediction = linreg.predict(4)\n",
    "\n",
    "print(\"The predicted value is {}\".format(prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
